---
title: "Regression and Predictions"
output:
  html_notebook: default
---

## Contents:

This notebook contains the following topics:

-   [Linear Regression]

    -   [Simple Linear Regression]
    -   [Multiple Linear Regression]

-   [Real-world Example using Regression]

-   [Factor Variables]

-   [Regression Diagnostics - Outliers, Influential Values, Correlated Errors]

-   [Polynomial and Spline Regression]

-   [Additional Points to Remember]

## Linear Regression

[**Aim**]{.ul}: to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, to estimate the value of the response (outcome of dependent) variable $y$, when only the predictors (explanatory or independent variables) $x$ values are known.

Given a data set of $n$ data points, $\{y_{i},\,x_{i1},\ldots ,x_{ip}\}_{i=1}^{n},$ the linear model with $p$-vector of predictors $x$ is:

${y}_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+ \epsilon_{i} = \mathbf{x}_{i}^{\mathsf {T}}{\boldsymbol {\beta }} + \epsilon_{i},\qquad i=1,\ldots ,n,$

where $\mathbf{x}_{i}^{\mathsf {T}} \beta$ is the inner product between vectors $\mathbf{x}_i$ and $\boldsymbol {\beta}$ and $\epsilon$ is the error variable, an unobserved random variable that adds 'noise' to the linear relationship.

In matrix notation, ${\mathbf{y = X}} {\boldsymbol{\beta}} + {\bf{\epsilon}}$, where,

$\mathbf{y} = \begin{pmatrix} y_1 \\ y_2 \\.\\.\\.\\ y_n \end{pmatrix}, \qquad {\boldsymbol {\beta}} = \begin{pmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\.\\.\\.\\ \beta_p \end{pmatrix}, \qquad {\bf{\epsilon}} = \begin{pmatrix} \epsilon_1 \\ \epsilon_2 \\.\\.\\.\\ \epsilon_n \end{pmatrix}$

${\bf{X}} = \begin{pmatrix} \bf{x}_1^{\mathsf{T}} \\ \bf{x}_2^{\mathsf{T}} \\.\\.\\.\\ \bf{x}_n^{\mathsf{T}} \end{pmatrix} = \begin{pmatrix} 1 & x_{11} & \ldots & x_{1p} \\ 1 & x_{21} & \ldots & x_{2p} \\. & . & \ldots & .\\. & . & \ldots & .\\. & . & \ldots & . \\ 1 & x_{n1} & \ldots & x_{np} \end{pmatrix}$

Summarize:

-   $\mathbf{X}$ - independent predictors
-   $\mathbf{y}$ - dependent response
-   $\boldsymbol{\beta}$ - regression coefficients
-   linear relationship between the response function and regression coefficients
-   distribution of $\mathbf{X}$ is arbitrary
-   distribution of estimated coefficients, $\widehat{\boldsymbol{\beta}}$ will depend on the distribution of predicted responses, $\widehat{\mathbf{y}}$

The best fitted model - one which minimizes the sum of the prediction errors/residuals, i.e., minimizes $\sum_{i=0}^n e_i = \sum_{i=0}^n (y_i - \widehat{y}_i)^2$

(i.e., minimizes the least squared error). Here $\widehat{y}_i$ are the predicted responses.

#### [Linear Regression has the following assumptions:]{.ul}

-   mean of the response at each value of the predictor, $x_i$, is a *linear* function of the $x_i$.
-   errors/residuals, $\epsilon_i$, are *identically and independently distributed* as normal distribution, with mean 0 and equal (unknown) variance.
-   errors, $\epsilon_i$, at each value of $x_i$, are *normally distributed.*
-   responses, $y_i$ and therefore, $\beta_i$, are *normally distributed*.

### Simple Linear Regression

Simple linear regression attempts to model the data in the form of the best fitting line:

${y}_i = \beta_0 + \beta_1 x_i + \epsilon_i \qquad i = 1,…, n$

This is the equation of a straight line, recall $y = mx +c$, where $m$ is the slope and $c$ is the intercept.

Minimization $\rightarrow$ take derivatives of $\sum_{i=1}^n e_i$ w.r.t $\beta_0$ and $\beta_1$, set to 0 and solve for $\beta_0$ and $\beta_1$.

$\widehat\beta_0 = \overline{y} - \widehat\beta_1 \overline{x}$

$\widehat\beta_1 = \frac{\sum_{i=1}^n (x_i - \overline{x} )(y_i - \overline{y} )}{\sum_{i=1}^n (x_i - \overline{x})^2}$

where,

$\overline{x}$ is the mean of all of the x-values

$\overline{y}$ is the mean of all of the y-values

$\widehat\beta_0$ tells the estimated regression equation at $x = 0$, if the 'scope of the model' includes $x = 0$, otherwise, $\widehat\beta_0$ is not meaningful.

$\widehat\beta_1$ is the amount by which the mean response vary for every one unit increase in $x$.

### Multiple Linear Regression

Multiple linear regression is a generalized version of simple regression, where more than one independent variables are used. . The basic model for multiple linear regression is as discussed above:

${y}_{i}=\beta _{0}+\beta _{1}x_{i1}+\cdots +\beta _{p}x_{ip}+ \epsilon_{i} = \mathbf{x}_{i}^{\mathsf {T}}{\boldsymbol {\beta }} + \epsilon_{i},\qquad i=1,\ldots ,n,$

The solution for the coefficients can be easily calculated using the matrix formulation: $\hat{\boldsymbol\beta} = (\mathbf{x}^T \mathbf{x})^{-1} \mathbf{x}^T \mathbf{y}$

Generally, all real-world problems involve multiple predictors.

## Real-world Example using Regression

Data: New York air quality data in R ([details about dataset](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/airquality.html))

-   Ozone: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island

-   Solar.R: Solar radiation in Langleys in the frequency band 4000--7700 Angstroms from 0800 to 1200 hours at Central Park

-   Wind: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport

-   Temp: Maximum daily temperature in degrees Fahrenheit at LaGuardia Airport

-   Month: Numeric value of Month (1--12)

-   Day: Day of month (1--31).

There are 6 variables - our target $\rightarrow$ attribute 'ozone'.

$lm(formula,data)$ function creates the relationship model between the predictor and the response variable, where the parameters used are -

-   formula is a symbol presenting the relation between x and y.
-   data is the vector on which the formula will be applied.

$predict(object, newdata)$ function is used for prediction, where the parameters used −

-   object is the formula which is already created using the lm() function.
-   newdata is the vector containing the new value for predictor variable.

```{r}
data('airquality')
str(airquality)
```

```{r}
head(airquality)
```

#### Exploratory Analysis and Pre-processing the data

Check for missing values in the dataset.

```{r}
mapply(anyNA, airquality)
```

There are some missing values in columns 'Ozone' and 'Solar.R'. Let's impute these by replacing the *'NA'* with monthly average.

```{r}
for (i in 1:nrow(airquality)){
  if(is.na(airquality[i,'Ozone'])){
    airquality[i,'Ozone'] = mean(airquality[
      which(airquality[, 'Month'] == airquality[i, 'Month']), 
            'Ozone'],na.rm = TRUE)
  }
  
  if(is.na(airquality[i,'Solar.R'])){
    airquality[i,'Solar.R'] = mean(airquality[
      which(airquality[, 'Month'] == airquality[i, 'Month']), 
            'Solar.R'],na.rm = TRUE)
  }
  
}
```

```{r}
head(airquality)
```

#### Simple Regression on the data

Since simple regression requires only one predictor, let's take 'Temp' as the predictor for the target 'Ozone'.

```{r}
# select predictor attribute
x =  airquality[, 'Temp']
                
# select target attribute
y = airquality[, 'Ozone'] 

# split into train-test set
library(caTools)
x_train = subset(x, sample.split(airquality, SplitRatio = 0.7) == TRUE)
y_train = subset(y, sample.split(airquality, SplitRatio = 0.7) == TRUE)

x_test = subset(x, sample.split(airquality, SplitRatio = 0.7) == FALSE)
y_test = subset(y, sample.split(airquality, SplitRatio = 0.7) == FALSE)
```

```{r}

simple_model  = lm(y_train~x_train)

# provides regression line coefficients, i.e., slope and y-intercept
simple_model
```

```{r}

# scatter plot between x and y
plot(y_train~x_train)
# plot the regression line
abline(simple_model, col = 'blue', lwd = 2)
```

```{r}
print(predict(simple_model))
```

```{r}

library(ggplot2)

p = ggplot(data = airquality, aes(x, y)) + geom_point() +
stat_smooth(method = 'lm', col = 'dodgerblue3') +
theme(panel.background = element_rect(fill = 'white'),
axis.line.x=element_line(),
axis.line.y=element_line()) + ggtitle('Linear Model Fitted to Data')

p + labs(x = 'Temp', y = 'Ozone')
```

The gray shading around the line represents a confidence interval of 0.95, the default for the stat_smooth() function, which smoothes data to make patterns easier to visualize. This 0.95 confidence interval is the probability that the true linear model for the temperature and ozone will lie within the confidence interval of the regression model fitted to the data. There is still variability within the observations, even though the model fits the data well.

```{r}
ggplot(data = airquality, aes(simple_model$residuals)) +
geom_histogram(bins = 50, color = 'black', fill = 'grey') +
theme(panel.background = element_rect(fill = 'white'),
axis.line.x=element_line(),
axis.line.y=element_line()) +
ggtitle('Histogram for Model Residuals')

```

## Factor Variables

text

## Regression Diagnostics - Outliers, Influential Values, Correlated Errors

text

## Polynomial and Spline Regression

text

## Additional Points to Remember

Over-fitting and under-fitting are two extreme ends of fitting a model to the data observations.

-   Under-fitting: when model doesn't capture the underlying trend from the data points. e.g.: trying to model a non-linear relationship with a linear model. This scenario is the case of high bias and low variance.
-   Over-fitting: when model tries to fit all the data points, that it starts capturing the noise. e.g.: trying to model a quadratic relationship with a 5th order polynomial. This is the case of low bias and high variance.

\- Bias: it is the error from incorrect assumptions in the learning algorithm.

\- Variance: it is the error from sensitivity to small fluctuations in the training set.

Frequently used techniques to reduce under-fitting:

-   Increase model complexity
-   Increase number of features, perform feature engineering
-   Remove noise from the data
-   Increase the number of epochs or increase the duration of training to get better results.

Frequently used techniques to reduce over-fitting:

-   Increase training data
-   Reduce model complexity
-   Early stopping during the training phase (keep an eye over the loss over the training period as soon as loss begins to increase stop training).
-   Ridge Regularization and Lasso Regularization - penalize for additional complexity

## References

1.  [Penn State STAT 462 Applied Regression Analysis](https://online.stat.psu.edu/stat462/node/77/)
2.  [Khan Academy](https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/assessing-fit-least-squares-regression/v/influential-points-regression))
3.  [Cornell Machine Learning Course](https://www.cs.cornell.edu/courses/cs4780/2018fa/)
4.  [CMU Data Analytics Course](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf)
